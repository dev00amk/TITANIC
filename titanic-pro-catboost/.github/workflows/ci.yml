name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run weekly on Sunday at 2 AM UTC
    - cron: '0 2 * * 0'

env:
  PYTHON_VERSION: '3.9'
  
jobs:
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Run ruff
      run: ruff check src/ tests/
    
    - name: Run black
      run: black --check src/ tests/
    
    - name: Run isort
      run: isort --check-only src/ tests/
    
    - name: Run mypy
      run: mypy src/
      continue-on-error: true  # Don't fail CI on type errors initially
    
    - name: Upload code quality artifacts
      uses: actions/upload-artifact@v3
      if: failure()
      with:
        name: code-quality-report
        path: |
          .mypy_cache/
          .ruff_cache/

  security:
    name: Security Scanning
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit
        pip install -e .
    
    - name: Run safety check
      run: safety check --json --output safety-report.json
      continue-on-error: true
    
    - name: Run bandit security scan
      run: bandit -r src/ -f json -o bandit-report.json
      continue-on-error: true
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          safety-report.json
          bandit-report.json

  test:
    name: Test Suite
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.8', '3.9', '3.10', '3.11']
        exclude:
          # Reduce matrix to save CI minutes
          - os: windows-latest
            python-version: '3.8'
          - os: macos-latest
            python-version: '3.8'
          - os: windows-latest
            python-version: '3.10'
          - os: macos-latest
            python-version: '3.11'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    
    - name: Install system dependencies (Ubuntu)
      if: matrix.os == 'ubuntu-latest'
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Create test data
      run: |
        mkdir -p data
        python -c "
        import pandas as pd
        import numpy as np
        np.random.seed(42)
        data = {
            'PassengerId': range(1, 892),
            'Survived': np.random.randint(0, 2, 891),
            'Pclass': np.random.choice([1, 2, 3], 891),
            'Name': [f'Person_{i}, Mr. Test' for i in range(891)],
            'Sex': np.random.choice(['male', 'female'], 891),
            'Age': np.random.uniform(1, 80, 891),
            'SibSp': np.random.randint(0, 4, 891),
            'Parch': np.random.randint(0, 3, 891),
            'Ticket': [f'TICKET_{i}' for i in range(891)],
            'Fare': np.random.uniform(5, 500, 891),
            'Cabin': [f'A{i}' if i % 3 == 0 else None for i in range(891)],
            'Embarked': np.random.choice(['S', 'C', 'Q'], 891)
        }
        pd.DataFrame(data).to_csv('data/train.csv', index=False)
        "
    
    - name: Run tests
      run: pytest tests/ -v --tb=short --durations=10
    
    - name: Run tests with coverage
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.9'
      run: |
        pytest tests/ --cov=src --cov-report=xml --cov-report=html --cov-report=term-missing
    
    - name: Upload coverage to Codecov
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.9'
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false
    
    - name: Upload test artifacts
      if: failure()
      uses: actions/upload-artifact@v3
      with:
        name: test-artifacts-${{ matrix.os }}-${{ matrix.python-version }}
        path: |
          htmlcov/
          .pytest_cache/
          test-results.xml

  integration-test:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [code-quality, test]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Setup test data
      run: make setup-data
    
    - name: Run integration tests
      run: pytest tests/test_integration.py -v --tb=short
    
    - name: Test training pipeline
      run: |
        timeout 300 python -m src.modeling.train_cv || echo "Training test completed"
    
    - name: Upload integration test artifacts
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: integration-test-artifacts
        path: |
          artifacts/
          logs/

  red-team-validation:
    name: Red Team Validation Gates
    runs-on: ubuntu-latest
    needs: [integration-test]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Setup test data
      run: make setup-data
    
    - name: Run shuffled target validation
      id: shuffled_target
      run: |
        echo "Running shuffled target validation..."
        python -c "
        import sys, json
        sys.path.append('src')
        from validation.shuffled_target_check import run_shuffled_target_check
        import yaml
        
        with open('configs/train.yaml') as f:
            config = yaml.safe_load(f)
        config['catboost']['iterations'] = 100  # Fast for CI
        config['cv']['n_seeds'] = 1
        
        try:
            results = run_shuffled_target_check(config)
            cv_mean = results['shuffled_target_cv_mean']
            
            print(f'Shuffled target CV: {cv_mean:.4f}')
            
            # CI Gate: Must be in [0.48, 0.52]
            if not (0.48 <= cv_mean <= 0.52):
                print(f'❌ FAIL: Shuffled target CV {cv_mean:.4f} outside [0.48, 0.52] - potential leakage')
                sys.exit(1)
            else:
                print(f'✅ PASS: Shuffled target CV {cv_mean:.4f} in valid range')
                
            with open('shuffled_target_results.json', 'w') as f:
                json.dump(results, f)
        except Exception as e:
            print(f'Error in shuffled target validation: {e}')
            sys.exit(1)
        "
    
    - name: Run adversarial validation
      id: adv_validation
      run: |
        echo "Running adversarial validation..."
        python -c "
        import sys, json
        sys.path.append('src')
        from validation.adv_validation import run_adversarial_validation
        import yaml
        
        with open('configs/train.yaml') as f:
            config = yaml.safe_load(f)
        
        try:
            results = run_adversarial_validation(config)
            adv_auc = results['adversarial_auc']
            
            print(f'Adversarial AUC: {adv_auc:.4f}')
            
            # CI Gate: AUC must be <= 0.60
            if adv_auc > 0.60:
                print(f'❌ FAIL: Adversarial AUC {adv_auc:.4f} > 0.60 - significant distribution shift detected')
                print('Top shift features:')
                for feat in results['top_shift_features'][:3]:
                    print(f'  {feat[\"feature\"]}: {feat[\"importance\"]:.4f}')
                sys.exit(1)
            else:
                print(f'✅ PASS: Adversarial AUC {adv_auc:.4f} <= 0.60')
                
            with open('adv_validation_results.json', 'w') as f:
                json.dump(results, f)
        except Exception as e:
            print(f'Error in adversarial validation: {e}')
            sys.exit(1)
        "
    
    - name: Run calibration validation
      id: calibration
      run: |
        echo "Running calibration validation..."
        python -c "
        import sys, json, os
        sys.path.append('src')
        
        # First ensure we have OOF predictions
        from modeling.train_cv import TrainingPipeline
        from validation.calibration import run_calibration_analysis
        from pathlib import Path
        import yaml
        
        with open('configs/train.yaml') as f:
            config = yaml.safe_load(f)
        config['catboost']['iterations'] = 100
        config['cv']['n_seeds'] = 1
        
        try:
            # Generate OOF predictions if needed
            artifacts_dir = Path('artifacts')
            artifacts_dir.mkdir(exist_ok=True)
            
            oof_path = artifacts_dir / 'oof_predictions.npy'
            if not oof_path.exists():
                print('Generating OOF predictions for calibration test...')
                pipeline = TrainingPipeline(config)
                pipeline.run()
            
            # Run calibration analysis
            results = run_calibration_analysis(config, artifacts_dir)
            
            brier_pre = results['pre_calibration']['brier_score']
            brier_post = results['post_calibration']['brier_score']
            ece_pre = results['pre_calibration']['ece']
            ece_post = results['post_calibration']['ece']
            
            print(f'Calibration - Brier: {brier_pre:.4f} -> {brier_post:.4f}')
            print(f'Calibration - ECE: {ece_pre:.4f} -> {ece_post:.4f}')
            
            # CI Gate: Calibration should not worsen
            if brier_post > brier_pre + 0.01 or ece_post > ece_pre + 0.01:
                print(f'❌ FAIL: Calibration worsened - Brier: {brier_post-brier_pre:+.4f}, ECE: {ece_post-ece_pre:+.4f}')
                sys.exit(1)
            else:
                print(f'✅ PASS: Calibration maintained or improved')
                
            with open('calibration_results.json', 'w') as f:
                json.dump({k: float(v) if hasattr(v, 'item') else v for k, v in results.items() if k not in ['pre_calibration', 'post_calibration']}, f)
        except Exception as e:
            print(f'Error in calibration validation: {e}')
            sys.exit(1)
        "
    
    - name: Run slice fairness validation
      id: slice_validation
      run: |
        echo "Running slice fairness validation..."
        python -c "
        import sys, json
        sys.path.append('src')
        from validation.slice_metrics import run_slice_analysis
        from pathlib import Path
        import yaml
        
        with open('configs/train.yaml') as f:
            config = yaml.safe_load(f)
        
        try:
            artifacts_dir = Path('artifacts')
            slice_df = run_slice_analysis(config, artifacts_dir)
            
            global_acc = slice_df[slice_df['slice'] == 'Global']['accuracy'].iloc[0]
            min_slice_acc = slice_df[slice_df['slice'] != 'Global']['accuracy'].min()
            
            print(f'Global accuracy: {global_acc:.4f}')
            print(f'Min slice accuracy: {min_slice_acc:.4f}')
            print(f'Accuracy gap: {min_slice_acc - global_acc:+.4f}')
            
            # CI Gate: Min slice accuracy >= global - 0.05
            if min_slice_acc < global_acc - 0.05:
                print(f'❌ FAIL: Min slice accuracy {min_slice_acc:.4f} < global - 0.05 ({global_acc - 0.05:.4f})')
                worst_slice = slice_df[slice_df['accuracy'] == min_slice_acc].iloc[0]
                print(f'Worst slice: {worst_slice[\"slice\"]} (n={worst_slice[\"n_samples\"]})')
                sys.exit(1)
            else:
                print(f'✅ PASS: All slices within acceptable range')
                
            slice_summary = {
                'global_accuracy': float(global_acc),
                'min_slice_accuracy': float(min_slice_acc),
                'accuracy_gap': float(min_slice_acc - global_acc),
                'n_slices': len(slice_df) - 1
            }
            with open('slice_validation_results.json', 'w') as f:
                json.dump(slice_summary, f)
        except Exception as e:
            print(f'Error in slice validation: {e}')
            sys.exit(1)
        "
    
    - name: Memory and time budget validation
      id: budget_validation
      run: |
        echo "Running budget validation..."
        python -c "
        import time, psutil, os, sys
        
        # Monitor training pipeline execution
        start_time = time.time()
        process = psutil.Process()
        start_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        print(f'Starting memory: {start_memory:.1f} MB')
        
        # Run a quick training cycle
        exit_code = os.system('timeout 120 python -m src.modeling.train_cv > /dev/null 2>&1')
        
        end_time = time.time()
        current_memory = process.memory_info().rss / 1024 / 1024
        
        wall_time = end_time - start_time
        memory_used = current_memory - start_memory
        
        print(f'Wall time: {wall_time:.1f}s')
        print(f'Memory delta: {memory_used:.1f} MB')
        
        # CI Gates: Wall time < 150s, Memory < 500MB
        if wall_time > 150:
            print(f'❌ FAIL: Wall time {wall_time:.1f}s > 150s budget')
            sys.exit(1)
        if memory_used > 500:
            print(f'❌ FAIL: Memory usage {memory_used:.1f}MB > 500MB budget')
            sys.exit(1)
            
        print(f'✅ PASS: Within time and memory budgets')
        "
    
    - name: Upload validation artifacts
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: red-team-validation-results
        path: |
          *_results.json
          artifacts/
          reports/

  performance-test:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [test]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-benchmark
    
    - name: Run performance tests
      run: pytest tests/ -k "performance" --benchmark-only --benchmark-json=benchmark-results.json
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: benchmark-results.json

  docker-build:
    name: Docker Build
    runs-on: ubuntu-latest
    needs: [code-quality, test]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build Docker image
      run: |
        docker build -t titanic-catboost:${{ github.sha }} .
        docker tag titanic-catboost:${{ github.sha }} titanic-catboost:latest
    
    - name: Test Docker image
      run: |
        docker run --rm titanic-catboost:latest python --version
        docker run --rm titanic-catboost:latest python -c "import pandas, numpy, catboost; print('Dependencies OK')"
    
    - name: Save Docker image
      if: github.ref == 'refs/heads/main'
      run: |
        docker save titanic-catboost:latest | gzip > titanic-catboost.tar.gz
    
    - name: Upload Docker image artifact
      if: github.ref == 'refs/heads/main'
      uses: actions/upload-artifact@v3
      with:
        name: docker-image
        path: titanic-catboost.tar.gz

  documentation:
    name: Documentation Build
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install sphinx sphinx-rtd-theme
    
    - name: Build documentation
      run: |
        mkdir -p docs
        echo "# Titanic CatBoost Pipeline Documentation" > docs/README.md
        echo "This is a placeholder for project documentation." >> docs/README.md
        echo "## API Reference" >> docs/README.md
        echo "Auto-generated API documentation would go here." >> docs/README.md
    
    - name: Upload documentation
      uses: actions/upload-artifact@v3
      with:
        name: documentation
        path: docs/

  release:
    name: Release
    runs-on: ubuntu-latest
    needs: [code-quality, test, integration-test, red-team-validation, docker-build]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Generate version
      id: version
      run: |
        VERSION=$(date +%Y.%m.%d)-${GITHUB_SHA::8}
        echo "version=$VERSION" >> $GITHUB_OUTPUT
        echo "Generated version: $VERSION"
    
    - name: Create release tag
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git tag -a v${{ steps.version.outputs.version }} -m "Release v${{ steps.version.outputs.version }}"
    
    - name: Download artifacts
      uses: actions/download-artifact@v3
    
    - name: Create release
      uses: softprops/action-gh-release@v1
      with:
        tag_name: v${{ steps.version.outputs.version }}
        name: Release v${{ steps.version.outputs.version }}
        body: |
          ## Changes
          - Automated release from commit ${{ github.sha }}
          
          ## Artifacts
          - Docker image available in artifacts
          - Test coverage reports included
          - Documentation updated
          
          ## Installation
          ```bash
          git clone https://github.com/${{ github.repository }}.git
          cd titanic-pro-catboost
          make install-dev
          ```
        files: |
          docker-image/titanic-catboost.tar.gz
          benchmark-results/benchmark-results.json
          documentation/**
        draft: false
        prerelease: false
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  notify:
    name: Notifications
    runs-on: ubuntu-latest
    needs: [code-quality, test, integration-test, red-team-validation, docker-build]
    if: always()
    
    steps:
    - name: Notify on success
      if: needs.code-quality.result == 'success' && needs.test.result == 'success' && needs.red-team-validation.result == 'success'
      run: |
        echo "✅ CI Pipeline completed successfully!"
        echo "All quality checks and red-team validation passed."
    
    - name: Notify on failure
      if: needs.code-quality.result == 'failure' || needs.test.result == 'failure' || needs.red-team-validation.result == 'failure'
      run: |
        echo "❌ CI Pipeline failed!"
        echo "Please check the failed jobs and fix any issues."
        exit 1

  cleanup:
    name: Cleanup
    runs-on: ubuntu-latest
    needs: [release, notify]
    if: always()
    
    steps:
    - name: Clean up old artifacts
      uses: geekyeggo/delete-artifact@v2
      with:
        name: |
          test-artifacts-*
          integration-test-artifacts
        failOnError: false
    
    - name: Summary
      run: |
        echo "## CI/CD Pipeline Summary" >> $GITHUB_STEP_SUMMARY
        echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Code Quality | ${{ needs.code-quality.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Tests | ${{ needs.test.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Integration | ${{ needs.integration-test.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Red Team Validation | ${{ needs.red-team-validation.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Docker | ${{ needs.docker-build.result }} |" >> $GITHUB_STEP_SUMMARY